{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0855b561",
   "metadata": {},
   "source": [
    "# Integrate Modern Data Architectures with Generative AI and Interact Using Prompts for Querying SQL Databases and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26105729-b3e3-42d0-a583-8446fff89277",
   "metadata": {},
   "source": [
    "This notebook demonstrates how large language models (LLMs) that are accessible through [Amazon Bedrock](https://aws.amazon.com/bedrock/), such as Amazon Titan, interact with AWS databases, data stores, and third-party data warehousing solutions, such as [Amazon Athena](https://aws.amazon.com/athena/features/). This interaction is showcased by generating and running SQL queries, and by making requests to API endpoints. The LangChain framework, used to accomplish this demonstration, allows an LLM to interact with its environment and connect with other sources of data. The LangChain framework operates based on three principles: calling out to a language model, being data-aware, and being agentic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c8cc5-5104-44aa-bbce-ad3ca7562a29",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook focuses on establishing a connection to one data source, consolidating metadata, and returning fact-based data points in response to user queries by using LLMs and LangChain. The solution can be enhanced to add multiple data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310d6ea-2ee1-4979-bb5e-b65cb892c0cd",
   "metadata": {},
   "source": [
    "\n",
    "<img src='img-genai-sql-langchain-overall-solution.png' width=\"800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0986ea2-f794-431f-a341-b94f0118cb7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites:\n",
    "1. Use the Python 3 (Data Science 3.0) kernel.\n",
    "2. Install the required packages.\n",
    "3. Run the one-time setup by entering the user input parameters, copying the dataset, and running the AWS Glue crawler.\n",
    "3. Access to the LLM. The Amazon Titan Lite model is used in this notebook. For more information, see Amazon Titan Text models in the Amazon Bedrock User Guide at https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0297e0-f2dd-464b-9254-6693c45ebafc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution walkthrough:\n",
    "\n",
    "Step 1. Read into a pandas DataFrame the library data JSON file that was downloaded from Amazon S3.\n",
    "\n",
    "Step 2. Populate the AWS Glue Data Catalog by running an AWS Glue crawler on the staged JSON file. \n",
    "\n",
    "Step 3. To obtain information on the data schema, use the SQLAlchemy library to query the Data Catalog.\n",
    "\n",
    "Step 4. Define the functions to determine the best data channel to answer the user query, and then generate a response to the user query as a correctly formatted SQL statement.\n",
    "\n",
    "Step 5. Prompt the LLM through LangChain to determine the data channel. After determining the data channel, run the Langchain SQL database chain to convert 'text to sql', and then run the query against the source data channel. \n",
    "\n",
    "Finally, display the results.\n",
    "\n",
    "## Code cell 1 ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556eddc-8e45-4e42-9157-213316ec468a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install sqlalchemy==2.0.29\n",
    "!pip install langchain==0.1.19\n",
    "!pip install langchain-experimental==0.0.58\n",
    "!pip install PyAthena[SQLAlchemy]==3.8.2\n",
    "!pip install -U langchain-aws==0.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae3491-6f43-4f26-859d-77d9e969b5f7",
   "metadata": {},
   "source": [
    "## Code cell 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c153cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import PromptTemplate,SagemakerEndpoint,SQLDatabase,LLMChain\n",
    "from langchain_experimental.sql import SQLDatabaseChain, SQLDatabaseSequentialChain\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "from typing import Dict\n",
    "import time\n",
    "from langchain_aws import BedrockLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a725d9-9d23-4dcd-b49e-9722dbec7575",
   "metadata": {},
   "source": [
    "### Library dataset\n",
    "The dataset for this solution was originally stored in an Amazon Simple Storage Service (Amazon S3) bucket and downloaded locally as the file, s3_library_data.json. \n",
    "\n",
    "The dataset consists of the following columns:\n",
    "* book_id\n",
    "* title\n",
    "* author\n",
    "* genre\n",
    "* pub_date\n",
    "\n",
    "You can use the following commands to load the JSON data into a pandas DataFrame and view the data.\n",
    "\n",
    "## Code cell 3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c360556-fcb4-40d2-8293-fcc806c33acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "library_df = pd.read_json('s3_library_data.json',lines=True)\n",
    "library_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb560be7-cdea-4ff8-9230-0679252ecf5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## One-time setup\n",
    "### AWS CloudFormation outputs\n",
    "\n",
    "Some of the resources needed for this notebook have already been created for you as part of lab deployment. These preprovisioned resources include an S3 bucket to store data, AWS Glue databases, and AWS Glue crawlers. \n",
    "\n",
    "In order to proceed with the process, you first need to set some variables:\n",
    "* Lab files bucket\n",
    "* Library database name\n",
    "* Library crawler name\n",
    "\n",
    "Note: Make sure to modify the following CloudFormation stack name.\n",
    "\n",
    "## Code cell 4 ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db978b97-6a82-4764-aaea-5b1d2d401f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update with the correct CloudFormation stack name.\n",
    "cloudformation_stack_name = '<LabStack>'\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(cloudformation_stack_name):\n",
    "    outputs = {}\n",
    "    for output in cfn_client.describe_stacks(StackName=cloudformation_stack_name)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "json_formatted_str = json.dumps(outputs, indent=2)\n",
    "print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d340f-b555-48eb-b0fa-d42abe9fd352",
   "metadata": {},
   "source": [
    "## Code cell 5 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab506d-e40d-48ab-af71-82ebb78445c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following variables define the AWS Glue Data Catalog database and crawler to use.\n",
    "\n",
    "## DIY note ##  You must update some of the following variables as part of the later DIY section of this solution.\n",
    "data_file = 's3_library_data.json'\n",
    "lab_files_folder = 'library-data'\n",
    "glue_db_name = outputs['LibraryDatabaseName']\n",
    "glue_crawler_name = outputs['LibraryCrawlerName']\n",
    "\n",
    "# The following variable does not need to be changed.\n",
    "lab_files_bucket = outputs['LabfilesBucketName']\n",
    "\n",
    "print(lab_files_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5c0b3-d32e-41ff-85fe-dc80586fa737",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stage data\n",
    "\n",
    "Use the following command to copy the library JSON data to the S3 bucket's library-data folder.\n",
    "\n",
    "Note: In the later DIY section, you must modify some of the previously initialized variables.\n",
    "\n",
    "## Code cell 6 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedd61d-9c21-45b4-b35e-69e5cd047f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following command does not need to be changed.\n",
    "\n",
    "!aws s3 cp {data_file} s3://{lab_files_bucket}/{lab_files_folder}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6344f3f-40da-4ffb-8b4f-b27c52e1eb02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the AWS Glue crawler\n",
    "\n",
    "The crawler runs and crawls the specified S3 bucket and folder. The crawler then creates tables based on the data found. The crawled data is available for querying through Amazon Athena, or nontechnical users can use the power of LLMs to convert text questions into SQL.\n",
    "\n",
    "## Code cell 7 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe28c2a-60c4-400a-9693-395e304c5164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('glue')\n",
    "\n",
    "print(\"About to start running the crawler: \", glue_crawler_name)\n",
    "\n",
    "try:\n",
    "    response = client.start_crawler(Name=glue_crawler_name )\n",
    "    print(\"Successfully started crawler. The crawler may take 2-5 mins to detect the schema.\")\n",
    "    while True:\n",
    "        # Get the crawler status.\n",
    "        response = client.get_crawler(Name=glue_crawler_name)\n",
    "         # Extract the crawler state.\n",
    "        status = response['Crawler']['State']\n",
    "        # Print the crawler status.\n",
    "        print(f\"Crawler '{glue_crawler_name}' status: {status}\")\n",
    "        if status == 'STOPPING':  # Replace 'READY' with the desired completed state.\n",
    "            break  # Exit the loop if the desired state is reached.\n",
    "\n",
    "        time.sleep(10)  # Sleep for 10 seconds before checking the status again.\n",
    "    \n",
    "except:\n",
    "    print(\"error in starting crawler. Check the logs for the error details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132ffc3-6947-49b6-b627-fae3df870b88",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: Before proceeding to the next step, check to confirm that the crawler status changed from RUNNING to STOPPING."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d1d0e-33fb-46ca-b82f-6294ea867cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1 - Connect to databases by using SQLAlchemy. \n",
    "\n",
    "LangChain uses SQLAlchemy to connect to SQL databases. SQLDatabaseChain can be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. For more information about requirements for connecting to your database, see the SQLAlchemy documentation. \n",
    "\n",
    "Note: The following code establishes a database connection for data sources and LLMs. Note that the solution will work only if the database connection for your sources is defined in the next code cell. For more information, see the previous Prerequisites section.\n",
    "\n",
    "## Code cell 8 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583cade",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the AWS Region.\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "## Athena variables\n",
    "connathena=f\"athena.{region}.amazonaws.com\" \n",
    "portathena='443'                                         # Update, if port is different\n",
    "schemaathena=glue_db_name                                # from user defined params\n",
    "s3stagingathena=f's3://{lab_files_bucket}/athenaresults/' # from cfn params\n",
    "wkgrpathena='primary'                                    # Update, if workgroup is different\n",
    "\n",
    "## Create the Athena connection string.\n",
    "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\"\n",
    "\n",
    "## Create the Athena SQLAlchemy engine.\n",
    "engine_athena = create_engine(connection_string, echo=False)\n",
    "dbathena = SQLDatabase(engine_athena)\n",
    "\n",
    "gdc = [schemaathena]\n",
    "print(\"Connection to Athena database succeeded: \", gdc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea21757-b08a-438b-a5a7-79d85a9a9085",
   "metadata": {},
   "source": [
    "### Step 2 - Generate dynamic prompt templates.\n",
    "Build a consolidated view of the AWS Glue Data Catalog by combining metadata stored for all the databases in pipe-delimited format.\n",
    "\n",
    "## Code cell 9 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3373d-9285-4fab-81b5-51e5364590b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dynamic prompts to populate the AWS Glue Data Catalog.\n",
    "# Harvest AWS Glue crawler metadata.\n",
    "\n",
    "def parse_catalog():\n",
    "    # Connect to the Data Catalog.\n",
    "    columns_str = 'database|table|column_name'\n",
    "    \n",
    "    # Define the AWS Glue cient.\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    for db in gdc:\n",
    "        response = glue_client.get_tables(DatabaseName =db)\n",
    "        for tables in response['TableList']:\n",
    "            # Classification in the response for S3 and other databases is different. Set classification based on the response location.\n",
    "            if tables['StorageDescriptor']['Location'].startswith('s3'):  classification='s3' \n",
    "            else:  classification = tables['Parameters']['classification']\n",
    "            for columns in tables['StorageDescriptor']['Columns']:\n",
    "                    dbname,tblname,colname=tables['DatabaseName'],tables['Name'],columns['Name']\n",
    "                    columns_str = columns_str+f'\\n{dbname}|{tblname}|{colname}'\n",
    "    return columns_str\n",
    "\n",
    "glue_catalog = parse_catalog()\n",
    "\n",
    "print(glue_catalog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e6770-42c3-402b-a60e-9c21fb99d5f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3 - Define two functions.\n",
    "\n",
    "Define two functions that (1) determine the best data channel to answer the user query, and (2) generate a response to the user's query.\n",
    "\n",
    "## Code cell 10 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcc59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Bedrock LLM model to use.\n",
    "\n",
    "bedrock_model = 'amazon.titan-text-lite-v1'\n",
    "\n",
    "BEDROCK_CLIENT = boto3.client(\"bedrock-runtime\", 'us-east-1')\n",
    "\n",
    "# Amazon Titan does not require any inference modifiers.\n",
    "if bedrock_model == 'amazon.titan-text-lite-v1':\n",
    "    inference_modifier = {}\n",
    "else:\n",
    "    inference_modifier = {\"temperature\":0.0, \"max_tokens\":50}\n",
    "\n",
    "llm = BedrockLLM(model_id=bedrock_model, client=BEDROCK_CLIENT, model_kwargs = inference_modifier)\n",
    "\n",
    "def identify_channel(query):\n",
    "    prompt_template_titan = \"\"\"You are a SQL expert. Convert the below natural language question into a valid SQL statement. The schema has the structure below:\\n\n",
    "     \"\"\"+glue_catalog+\"\"\" \n",
    "     \\n\n",
    "     Here is the question to be answered:\\n\n",
    "     {query}\n",
    "     \\n\n",
    "     Provide the SQL query that would retrieve the data based on the natural language request.\\n\n",
    "     \n",
    "     \"\"\"\n",
    "    \n",
    "    prompt_template = prompt_template_titan\n",
    "    \n",
    "    # print(prompt_template)\n",
    "    \n",
    "    ## Define prompt 1.\n",
    "    PROMPT_channel = PromptTemplate(template=prompt_template, input_variables=[\"query\"])\n",
    "    \n",
    "    # Define the LLM chain.\n",
    "    llm_chain = LLMChain(prompt=PROMPT_channel, llm=llm)\n",
    "    # Run the query and save to generated texts.\n",
    "    generated_texts = llm_chain.run(query)\n",
    "    \n",
    "    # Set the channel from where the query can be answered.\n",
    "    if 's3' in generated_texts: \n",
    "            channel='db'\n",
    "            db=dbathena\n",
    "            print(\"SET database to athena\")\n",
    "    elif 'api' in generated_texts: \n",
    "            channel='api'\n",
    "            print(\"SET database to weather api\")        \n",
    "    else: raise Exception(\"User question cannot be answered by any of the channels mentioned in the catalog\")\n",
    "    # print(\"Step complete. Channel is: \", channel)\n",
    "    \n",
    "    return channel, db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63b7d1-163c-44c4-884c-47d31172c0e3",
   "metadata": {},
   "source": [
    "## Code cell 11 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb02e65-467c-4d60-9514-b9d30c432214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "\n",
    "    channel, db = identify_channel(query) # Call the identify channel function first.\n",
    "\n",
    "    _DEFAULT_TEMPLATE = \"\"\"\n",
    "    Here is a schema of a table:\n",
    "    <schema>\n",
    "    {table_info}\n",
    "    </schema>       \n",
    "    Run a SQL query to answer the question. Follow this format:\n",
    "    \n",
    "    SQLQuery: the correct SQL query. For example: select count ( * )  from s3_library_data where genre = 'Novel'\n",
    "    SQLResult: the result of the SQL query.\n",
    "    Answer: convert the SQLResult to a grammatically correct sentence.\n",
    "    \n",
    "    Here is question: {input}\"\"\"\n",
    "    \n",
    "    PROMPT_sql = PromptTemplate(\n",
    "        input_variables=[\"table_info\",\"input\"], template=_DEFAULT_TEMPLATE\n",
    "    )\n",
    "\n",
    "    \n",
    "    if channel=='db':\n",
    "        db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT_sql, verbose=True, return_intermediate_steps=False)\n",
    "        response=db_chain.run(query)\n",
    "    else: raise Exception(\"Unlisted channel. Check your unified catalog\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a92cd-e1b4-4feb-ab7a-f97030ba7f84",
   "metadata": {},
   "source": [
    "### Step 4 - Run the run_query function.\n",
    "\n",
    "Running the run_query function, in turn, calls the Langchain SQL database chain to convert 'text to sql', and then runs the query against the source data channel.\n",
    "\n",
    "The following samples are provided for test runs. Uncomment one query at a time to run it.\n",
    "## Code cell 12 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82599a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = \"\"\"How many books with a genre of Fantasy are in the library?\"\"\" \n",
    "# query = \"\"\"Find 3 books in the library with Tarzan in the title?\"\"\" \n",
    "# query = \"\"\"How many books by author Stephen King are in the library?\"\"\" \n",
    "query = \"\"\"How many total books are there in the library?\"\"\" \n",
    "\n",
    "response =  run_query(query)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(f'SQL and response from user query {query}  \\n  {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ed02a-dead-43fd-ad26-b15798ea4e79",
   "metadata": {},
   "source": [
    "### Step 5 - Do it yourself.\n",
    "\n",
    "Review the files in the left file browser. Review code cells 1–11 and pay close attention to the notes and comments.\n",
    "After updating any variables, rerun the necessary code cells so that a new database is created for Cars. \n",
    "\n",
    "The following samples are provided for test runs. Uncomment the query to run it.\n",
    "\n",
    "Note: You might need to run the next code cell twice if the first time the LLM uses double quotes.\n",
    "\n",
    "## Code cell DIY ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74482fe-9667-4c4b-ac4c-83557dde29fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = \"\"\"What car has the highest horsepower in the database?\"\"\" \n",
    "# query = \"\"\"Provide the make and price of the cheapest car?\"\"\" \n",
    "query = \"\"\"What is the average price of a car in the database?\"\"\" \n",
    "\n",
    "response =  run_query(query)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(f'SQL and response from user query {query}  \\n  {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d156adf-dbce-4192-b503-73ce40044b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
